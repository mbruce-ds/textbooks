\chapter{Getting Started}\,
\section{Insertion sort}

\qn{2.1-1} Using Figure 2.2 as a model, illustrate the operation of\newline \verb|INSERTION-SORT| on the array $A = \langle 31, 41, 59, 26, 41, 58\rangle$.

\sol First pass: key set to 41. \verb|A[1] = 31 < 41|, so the while loop breaks, and the second element stays put.

Second pass: key set to 59. Same steps as above.

Third pass: Key set to 26. \verb|A[3] = 59 > 26|, so set \verb|A[4] = 59|. \verb|A[2] =| \verb|41 > 26|, so set \verb|A[3] = 41|. \verb|A[1] = 31 > 26|, so set \verb|A[2] = 31|. Loop breaks, set \verb|A[1] = 26|.

Fourth pass: Key set to 41. \verb|A[4] = 59 > 41|, so set \verb|A[5] = 59|. \verb|A[3] =| \verb|41|, so the while loop breaks. Set \verb|[4] = 41|.

Final pass: Key set to 58. \verb|A[5] = 59 > 58|, so set \verb|A[6] = 59|. \verb|A[4] =| \verb|41 < 58|, so the while loop breaks. Set \verb|[5] = 58|. The final sorted array is $\langle 26, 31, 41, 41, 58, 59\rangle$.\qed

\qn{2.1-2} Rewrite the \verb|INSERTION-SORT| procedure to sort into nonincreasing instead of nondecreasing order.

\sol Rewrite the algorithm as follows.
\begin{Verbatim}[frame=single,numbers=left,samepage=true,label=REVERSE-INSERTION-SORT(A)]
for j = 2 to A.length
    key = A[j]
    // Insert A[j] into the sorted sequence A[1..j-1].
    i = j - 1
    while i > 0 and A[i] < key // The only change
        A[i + 1] = A[i]
        i = i - 1
    A[i + 1] = key
\end{Verbatim}

The implementation of this can be found in \newline\verb|code/<language>/chapter02/reverse_insertion_sort/|.\qed

\qn{2.1-3} Consider the \textbf{searching problem}:

\textbf{Input:} A sequence of $n$ numbers $A = \langle a_1, a_2, \dots, a_n \rangle$ and a value $v$.

\textbf{Output:} An index $i$ such that $v = A[i]$ or the special value \verb|NIL| if $v$ does not appear in $A$.

Write pseudocode for \textbf{linear search}, which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.

\sol The algorithm is as follows.

\begin{Verbatim}[frame=single,numbers=left,samepage=true,label={LINEAR-SEARCH(A,v)}]
for i = 1 to A.length
    if A[i] == v
        return i
return NIL
\end{Verbatim}

The loop invariant can be stated as follows: At the start of each iteration of the for loop, the subarray $A[1..i-1]$ does not contain the value $v$. We will show this invariant has the three necessary properties to prove the correctness of the algorithm. Prior to the first loop, the subarray $A[1..0]$ is an empty array, which clearly does not contain $v$. Suppose the invariant is true after loop $i$, that is, $A[1..i]$ does not contain $v$. Then on loop $i+1$, if $A[i+1] == v$, the algorithm will exit, returning $i+1$. If the algorithm does not exit, the loop iteration completes and thus $A[1..i+1]$ does not contain $v$. When the loop terminates without the algorithm prematurely returning, that means that $v$ is not contained in $A[1..n]$, and thus \verb|NIL| is returned. So the loop invariant fulfills the necessary properties to prove correctness of the algorithm. \qed

\qn{2.1-4} Consider the problem of adding two $n$-bit binary integers, stored in two $n$-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an $(n+1)$-element array $C$. State the problem formally and write pseudocode for adding the two integers.

\sol Define the problems as follows:

\textbf{Input:} Two sequences $A, B$ representing binary numbers, where the sequences are length $n$ and contain only 0 or 1. The endianness of $A,B$ should be defined, as it affects the algorithm to solve it. We will assume big-endian.

\textbf{Output:} A length $n+1$ sequence representing the binary sum $A+B$.

Write the algorithm solving the problem as follows:

\begin{Verbatim}[frame=single,numbers=left,samepage=true,label={ADD(A,B)}]
carry = 0
for i = A.length to 1
    sum = A[i] + B[i] + carry // Binary addition
    if sum % 10 == 1 // Sum either 01 or 11
        S[i+1] = 1
    if sum >= 10 // Sum either 10 or 11
        carry = 1
    else
        carry = 0
S[0] = carry
return S
\end{Verbatim}
\qed


\section{Analyzing algorithms}
\qn{2.2-1} Express the function $n^3 / 1000 - 100n^2 - 100n + 3$ in terms of $\Theta$-notation.

\sol Considering the informal definition of $\Theta$-notation in this chapter, we simply observe that  the $n^3 / 1000$ term is the leading term and dominates for large $n$, and drop the coefficient, expressing the function as $\Theta(n^3)$.\qed

\qn{2.2-2} Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n-1$ elements of $A$. Write pseudocode for this algorithm, which is known as \textbf{selection sort}. What loop invariant does this algorithm maintain? Why does it need to run for only the first $n-1$ elements, rather than for all $n$ elements? Give the best-case and worst-case running times of selection sort in $\Theta$-notation.

\sol
\begin{Verbatim}[frame=single,numbers=left,samepage=true,label={SELECTION-SORT(A)}]
for j = 1 to A.length - 1
    current_min = stored_val = A[j]
    stored_index = j
    for i = j + 1 to A.length
        if A[i] < current_min
            current_min = A[i]
            stored_index = i
    A[j] = current_min
    A[stored_index] = stored_val
\end{Verbatim}

See \verb|code/<language>/chapter02/selection_sort/| for implementation of this algorithm.

The loop invariant that the outer loop maintains is that the subarray $A[1..j]$ is sorted. Informally, this holds because the minimum element is selected to the first index, the second smallest element is selected to the second index, and so forth through the first $j$ elements. The loop only needs to run for the first $n-1$ elements because after $n-1$ iterations, the largest element is necessarily in the final index, as the $n-i$th largest elements are already all fixed to the $i$th indices. The best case runtime is $\Theta(n^2)$, since even with an input list that is already sorted, there is no way to early stop either for loop, as all elements need to be checked to confirm each candidate element is the current minimum. The worst case is equivalently $\Theta(n^2)$, as the same checks as in the best case are still performed, but the current minimum and stored index are updated on each increment of the $i$ index (in $\Theta$ notation, this only adds a linear term which is already accounted for by the nested loop).
\qed

\qn{2.2-3} Consider linear search again (see Exercise 2.1-3). How many elements of the input sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\Theta$ notation? Justify your answers.

\sol Assume the element to be checked is present in the array (otherwise no information is given on the ratio of present elements to non-present elements, and an average case is impossible to derive).  The average case is equal to $\sum_i^n p(i) \cdot i$, where $i$ is equal to each index. Since every index has equal probability to have the element, $p(i) = 1/n$. Thus, we have $\sum_i^n p(i) \cdot i = n(n+1) / (2n) = (n+1)/2$. In $\Theta$ notation, the average case is $\Theta(n)$ (strip away the coefficients / constants on the previous solution), and the worst case is $\Theta(n)$ (check $n$ indices).\qed

\qn{2.2-4} How can we modify almost any algorithm to have a good best-case running time?

\sol Have an initial check to verify if the input is the solution (e.g., if the input array to a sorting algorithm is sorted).\qed

\section{Designing algorithms}

\qn{2.3-1} Using Figure 2.4 as a model, illustrate the operation of merge sort on the array $A = \langle 3, 41, 52, 26, 38, 57, 9, 49 \rangle$.

\sol The initial array of $\langle 3, 41, 52, 26, 38, 57, 9, 49\rangle$ gets split up into arrays of length 1. Then, the arrays are paired off in order and merged together, to form $\langle 3, 41, 26, 52, 38, 57, 9, 49\rangle$. Then, pairs are combined into 4 to give $\langle 3, 26, 41, 52, 9, 38, 49, 57\rangle$. Then, the two sets of 4 are merged to get the final sorted array of $\langle3, 9, 26, 38, 41, 49, 52, 57\rangle$\qed

\qn{2.3-2} Rewrite the \verb|MERGE| procedure so that it does not use sentinels, instead stopping once either array $L$ or $R$ has had all its elements copied back into $A$ and then copying the remainder of the other array back into $A$.

\sol \begin{Verbatim}[frame=single,numbers=left,samepage=true,label={MERGE(A, p, q, r)}]
n_1 = q - p + 1
n_2 = r - q
let L[1..n_1 + 1] and R[1..n_2 + 1] be new arrays
for i = 1 to n_1
    L[i] = A[p + i - 1]
for j = 1 to n_2
    R[j] = A[q + j]

\end{Verbatim}
\qed

\qn{2.3-3} Use mathematical induction to show that when $n$ is an exact power of 2, the solution of the recurrence $$T(n) = \begin{cases} 2 & \text{if } n = 2, \\ 2T(n/2) + n & \text{if } n = 2^k \text{, for } k > 1 \end{cases}$$ is $T(n) = n \lg n$.

\sol Let $k = 1$. Then by definition of $T$, $T(2) = 2 = 2 \lg 2$.

Suppose the hypothesis is true for $k = m$. Let $k = m+1$. We find that $T(2^{m+1}) = 2T(2^{m+1}/2) + 2^{m+1} = 2T(2^m) + 2^{m+1} = 2\cdot 2^m \lg 2^m + 2^{m+1} = m\cdot 2^{m+1} + 2^{m+1} = (m+1)2^{m+1} = 2^{m+1} \lg 2^{m+1}$. This proves the inductive hypothesis, and thus the result is true for all $k \in \Z^+$, or when $n$ is a power of 2. \qed

\qn{2.3-4} We can express insertion sort as a recursive procedure as follows. In order to sort $A[1..n]$, we recursively sort $A[1..n-1]$ and then insert $A[n]$ into the sorted array $A[1..n-1]$. Write a recurrence for the worst-case running time of this recursive version of insertion sort.

\sol $$T(n) = \begin{cases}
    1 & \text{if } n=1, \\
    T(n-1) + (n-1)& \text{if } n \geq 2.
\end{cases}$$
This follows from considering the worst case of inserting $A[n]$ into sorted $A[1..n-1]$: needing to swap $A[n]$ with every index until it is at the first index. This yields $n-1$ swaps, in addition to all swaps to sort $A[1..n-1]$. Solving the recurrence is done by summing all integers from 1 to $n-1$, yielding $\Theta(n^2)$.\qed

\qn{2.3-5} Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The \textbf{binary search} algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\Theta(\lg n)$.

\sol \begin{Verbatim}[frame=single,numbers=left,samepage=true,label={BINARY-SEARCH(A, v, p, q)}]
if p == q and A[p] != v
    return NIL
midpoint = floor((p + q) / 2)
if A[midpoint] == v
    return midpoint
if A[midpoint] > v
    return BINARY-SEARCH(A, v, p, midpoint)
return BINARY-SEARCH(a, v, midpoint, q)
\end{Verbatim}
The worst case is when the element is not in the list, in which case the array size must halve until it is one element large and the element is not the value to search for, returning \verb|NIL|. Halving $A$ this way to a size of one element takes $\lg n$ steps, so the procedure takes $\Theta(\lg n)$ time.
\qed

\qn{2.3-6} Observe that the \textbf{while} loop of lines 5-7 of the \verb|INSERTION SORT| procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[1..j-1]$. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\Theta(n \lg n)$?

\sol It could be implemented, but it would not improve the running time to $\Theta(n \lg n)$. Once the index that $A[j]$ is to be inserted in is found, say index $i$, the elements of the subarray $A[i..j-1]$ still need to be shifted into $A[i+1..j]$. This will involve a nested loop that, in worst case, will still iterate over the entire subarray $A[1..j-1]$, so the running time will still be $\Theta(n \lg n)$.\qed

\qn{2.3-7*} Describe a $\Theta(n \lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$.

\sol Name this procedure \verb|PAIR-TO-SUM|. The pseudocode for this procedure is given by the following.
\begin{Verbatim}[frame=single,numbers=left,samepage=true,label={PAIR-TO-SUM(S, x)}]
n = S.size
for i=1 to n
    value = x - S[i]
    subarray = Union(S[1..i-1], S[i+1..n])
    j = BINARY-SEARCH(subarray, value, 1, n)
    if j != NIL
        return i, j
return NIL
\end{Verbatim}
This procedure iterates over the $n$ elements of $S$, binary searching for the unique element that would sum to $x$. Binary search is $\Theta(\lg n)$, and it is called $n$ times in worst case, so this procedure is $\Theta(n \lg n)$.
\qed

\section{Problems}

\qn{2-1} \textbf{Insertion sort on small arrays in merge sort}

Although merge sort runs in $\Theta(n \lg n)$ worst-case time and insertion sort runs in $\Theta(n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus, it makes sense to \textbf{coarsen} the leaves of the recursion by using insertion sort within the merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined.

\begin{enumerate}[(a)]
    \item Show that insertion sort can sort the $n/k$ sublists, each of length $k$, in $\Theta(nk)$ worst-case time.
    \item Show how to merge sublists in $\Theta(n \lg(n/k))$ worst-case time.
    \item Given that the modified algorithm runs in $\Theta(nk + n\lg(n/k))$ worst-case time, what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the same running time as standard merge sort, in terms of $\Theta$-notation?
    \item How should we choose $k$ in practice?
\end{enumerate}

\sol \begin{enumerate}[(a)]
    \item There exist $n/k$ sublists of length $k$. With insertion sort, each sublist takes $\Theta(k^2)$ time to sort. Since we do this $\Theta(k^2)$ operation $n/k$ times, we multiply to find the worst time for insertion sort to sort all sublists is $\Theta(nk)$.
    \item Perform the same steps as in normal merge sort, but since the branching tree of sublists is only $\lg (n/k)$ iterations deep instead of $ \lg n$, merging the sublists takes $\Theta(n \lg (n/k))$ time.
    \item The threshold is $k = \lg n$, at which point the first part of the sum will begin to dominate as $k$ gets larger.
    \item We can choose some constant value which isn't too large, for instance $k=64$.
\end{enumerate}\qed

\qn{2-2} \textbf{Correctness of bubblesort}

Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order.

\,

\begin{Verbatim}[frame=single,numbers=left,samepage=true,label={BUBBLESORT(A)}]
for i = 1 to A.length - 1
    for j = A.length to i + 1
        if A[j] < A[j - 1]
            exchange A[j] with A[j - 1]
\end{Verbatim}

\begin{enumerate}[(a)]
    \item Let $A'$ denote the output of \verb|BUBBLESORT|$(A)$. To prove that \verb|BUBBLESORT| is correct, we need to prove that it terminates and that \begin{equation}A'[1] \leq A'[2] \leq \cdots \leq A'[n],\end{equation} where $n = A.length$. In order to show that \verb|BUBBLESORT| actually sorts, what else do we need to prove?
\end{enumerate}

The next two parts will prove inequality (2.1).

\begin{enumerate}[(a)]
    \setcounter{enumi}{1}
    \item State precisely a loop invariant for the \textbf{for} loop in lines 2-4, and prove that this loop invariant holds. Your proof should use the structure of the loop invariant proof presented in this chapter.
    \item Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the \textbf{for} loop in lines 1-4 that will allow you to prove inequality (2.1). Your proof should use the structure of the loop invariant proof presented in this chapter.
    \item What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort?
\end{enumerate}

\sol \qed

\qn{2-3} \textbf{Correctness of Horner's rule}

The following code fragment implements Horner's rule for evaluating a polynomial

\begin{align*}
    P(x) &= \sum_{k=0}^n a_k x^k \\
    &= a_0 + x(a_1 + x(a_2 + \cdots + x(a_{n-1} + xa_n)\cdots )),
\end{align*}

Given the coefficients $a_0, a_1, \dots, a_n$ and a value for $x$:
\begin{Verbatim}[frame=single,numbers=left,samepage=true]
y = 0
for i = n downto 0
    y = a_i + x * y
\end{Verbatim}

\begin{enumerate}[(a)]
    \item In terms of $\Theta$-notation, what is the running time of this code fragment for Horner's rule?
    \item Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner's rule?
    \item Consider the following loop invariant: At the start of each iteration of the \textbf{for} loop of lines 2-3, $$y = \sum_{k=0}^{n - (i + 1)} a_{k+i+1}x^k.$$ Interpret a summation with no terms as equaling 0. Following the structure of the loop invariant proof presented in this chapter, use this loop invariant to show that, at termination, $y = \sum_{k=0}^n a_kx^k$.
    \item Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefficients $a_0, a_1, \dots, a_n$.
\end{enumerate}

\sol \qed

\qn{2-4} \textbf{Inversions}

Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i,j)$ is called an \textbf{inversion} of $A$.

\begin{enumerate}[(a)]
    \item List the five inversions of the array $\langle 2, 3, 8, 6, 1 \rangle$.
    \item What array with elements from the set $\{1, 2, \dots, n\}$ has the most inversions? How many does it have?
    \item What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.
    \item Give an algorithm that determines the number of inversions in any permutation on $n$ elements in $\Theta(n \lg n)$ worst-case time. (\textit{Hint:} Modify merge sort.)
\end{enumerate}

\sol \begin{enumerate}[(a)]
    \item The inversions are $(1, 5), (2, 5), (3, 5), (4, 5), (3, 4)$.
    \item The array with the most inversions is the reverse sorted array: $[n, n-1, \dots, 2, 1]$. It has $n-1$ inversions for the first element, $n-2$ for the second, and so forth, for a total of $1 + 2 + \dots + n-1 = n(n-1)/2$.
    \item Insertion sort works by iterating through each element in the array, undoing every inversion an element has with elements appearing prior to it in the array. Thus, if $I(A)$ is a formula for the number of inversions in an array, the running time of insertion sort is equal to $I(A) + \Theta(n)$, where the code to maintain the state of the procedure is $\Theta(n)$.
    \item Take merge sort, and add to a counter for every element added to the array from the right array before all elements from the left are depleted. Return this count for every subprocess, and add these counts together to get the total number of inversions.
\end{enumerate}\qed